{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1445    56.067063\n",
       "1446    56.056793\n",
       "1447    56.046522\n",
       "1448    56.036251\n",
       "1449    56.025980\n",
       "Name: C6, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1345     0.000000\n",
       "1346     0.202563\n",
       "1347     0.219981\n",
       "1348     1.016252\n",
       "1349     0.000000\n",
       "Name: C6, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 43ms/step - loss: 4226.8843 - val_loss: 3182.9910\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4144.8237 - val_loss: 3142.8215\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4098.9824 - val_loss: 3102.8140\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4053.3979 - val_loss: 3063.1433\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4008.1826 - val_loss: 3023.8411\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3963.3569 - val_loss: 2984.9126\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3918.9209 - val_loss: 2946.3542\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3874.8708 - val_loss: 2908.1619\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3831.2021 - val_loss: 2870.3320\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3787.9106 - val_loss: 2832.8606\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3744.9941 - val_loss: 2795.7444\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3702.4473 - val_loss: 2758.9810\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3660.2695 - val_loss: 2722.5669\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3618.4548 - val_loss: 2686.5002\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3577.0034 - val_loss: 2650.7778\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3535.9114 - val_loss: 2615.3979\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3495.1770 - val_loss: 2580.3574\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3454.7971 - val_loss: 2545.6548\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3414.7695 - val_loss: 2511.2871\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3375.0923 - val_loss: 2477.2524\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3335.7625 - val_loss: 2443.5486\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3296.7795 - val_loss: 2410.1731\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3258.1392 - val_loss: 2377.1240\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3219.8403 - val_loss: 2344.3994\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3181.8813 - val_loss: 2311.9968\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3144.2588 - val_loss: 2279.9143\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3106.9709 - val_loss: 2248.1494\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3070.0164 - val_loss: 2216.7004\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3033.3921 - val_loss: 2185.5654\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2997.0969 - val_loss: 2154.7424\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2961.1277 - val_loss: 2124.2292\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2925.4841 - val_loss: 2094.0229\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2890.1626 - val_loss: 2064.1228\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2855.1614 - val_loss: 2034.5260\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2820.4790 - val_loss: 2005.2316\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2786.1133 - val_loss: 1976.2368\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2752.0618 - val_loss: 1947.5398\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2718.3230 - val_loss: 1919.1382\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2684.8948 - val_loss: 1891.0309\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2651.7756 - val_loss: 1863.2153\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2618.9634 - val_loss: 1835.6896\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2586.4558 - val_loss: 1808.4520\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2554.2507 - val_loss: 1781.5013\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2522.3469 - val_loss: 1754.8340\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2490.7419 - val_loss: 1728.4500\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2459.4348 - val_loss: 1702.3466\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2428.4229 - val_loss: 1676.5214\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2397.7046 - val_loss: 1650.9729\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2367.2776 - val_loss: 1625.6997\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2337.1409 - val_loss: 1600.6997\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2302.0767 - val_loss: 1569.1038\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2267.9722 - val_loss: 1541.6315\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2235.2400 - val_loss: 1514.8344\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2203.2878 - val_loss: 1488.6986\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2172.0410 - val_loss: 1463.1328\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2141.3970 - val_loss: 1438.0629\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2111.2803 - val_loss: 1413.4374\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2081.6372 - val_loss: 1389.2181\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2052.4287 - val_loss: 1365.3802\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2023.6279 - val_loss: 1341.9028\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1995.2130 - val_loss: 1318.7690\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1967.1674 - val_loss: 1295.9683\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1939.4774 - val_loss: 1273.4882\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1912.1321 - val_loss: 1251.3201\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1885.1210 - val_loss: 1229.4572\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1858.4363 - val_loss: 1207.8915\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1832.0704 - val_loss: 1186.6180\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1806.0176 - val_loss: 1165.6306\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1780.2714 - val_loss: 1144.9252\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1754.8269 - val_loss: 1124.4962\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1729.6790 - val_loss: 1104.3403\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1704.8237 - val_loss: 1084.4531\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1680.2565 - val_loss: 1064.8309\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1655.9734 - val_loss: 1045.4706\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1631.9708 - val_loss: 1026.3691\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1608.2451 - val_loss: 1007.5224\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1584.7937 - val_loss: 988.9280\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1561.6125 - val_loss: 970.5828\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1538.6991 - val_loss: 952.4843\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1516.0503 - val_loss: 934.6297\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1493.6636 - val_loss: 917.0164\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1471.5361 - val_loss: 899.6418\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1449.6650 - val_loss: 882.5035\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1428.0482 - val_loss: 865.5991\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1406.6825 - val_loss: 848.9260\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1385.5664 - val_loss: 832.4827\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1364.6969 - val_loss: 816.2659\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1344.0718 - val_loss: 800.2743\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1323.6890 - val_loss: 784.5050\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1303.5464 - val_loss: 768.9556\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1283.6414 - val_loss: 753.6254\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1263.9720 - val_loss: 738.5113\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1244.5363 - val_loss: 723.6112\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1225.3322 - val_loss: 708.9237\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1206.3573 - val_loss: 694.4462\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1187.6099 - val_loss: 680.1767\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1169.0881 - val_loss: 666.1143\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1150.7898 - val_loss: 652.2556\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1132.7131 - val_loss: 638.5999\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1114.8560 - val_loss: 625.1447\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1097.2164 - val_loss: 611.8880\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1079.7928 - val_loss: 598.8283\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1062.5834 - val_loss: 585.9639\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1045.5858 - val_loss: 573.2929\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1028.7988 - val_loss: 560.8127\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1012.2204 - val_loss: 548.5226\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 995.8488 - val_loss: 536.4209\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 979.6819 - val_loss: 524.5048\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 963.7183 - val_loss: 512.7734\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 947.9564 - val_loss: 501.2246\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 932.3942 - val_loss: 489.8569\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 917.0302 - val_loss: 478.6686\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 901.8624 - val_loss: 467.6574\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 886.8892 - val_loss: 456.8221\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 872.1088 - val_loss: 446.1612\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 857.5198 - val_loss: 435.6729\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 843.1205 - val_loss: 425.3557\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 828.9094 - val_loss: 415.2073\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 814.8842 - val_loss: 405.2262\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 801.0436 - val_loss: 395.4116\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 787.3865 - val_loss: 385.7609\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 773.9108 - val_loss: 376.2728\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 760.6146 - val_loss: 366.9463\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 747.4969 - val_loss: 357.7790\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 734.5558 - val_loss: 348.7700\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 721.7899 - val_loss: 339.9165\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 709.1973 - val_loss: 331.2180\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 696.7767 - val_loss: 322.6729\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 684.5265 - val_loss: 314.2794\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 672.4451 - val_loss: 306.0357\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 660.5311 - val_loss: 297.9402\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 648.7829 - val_loss: 289.9923\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 637.1992 - val_loss: 282.1894\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 625.7781 - val_loss: 274.5309\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 614.5182 - val_loss: 267.0143\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 603.4181 - val_loss: 259.6390\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 592.4764 - val_loss: 252.4025\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 581.6910 - val_loss: 245.3041\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 571.0612 - val_loss: 238.3416\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 560.5850 - val_loss: 231.5145\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 550.2613 - val_loss: 224.8209\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 540.0884 - val_loss: 218.2586\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 530.0649 - val_loss: 211.8275\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 520.1893 - val_loss: 205.5247\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 510.4601 - val_loss: 199.3495\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 500.8762 - val_loss: 193.3009\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 491.4362 - val_loss: 187.3763\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 482.1381 - val_loss: 181.5753\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 472.9810 - val_loss: 175.8955\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 463.9630 - val_loss: 170.3364\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 455.0833 - val_loss: 164.8961\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 446.3398 - val_loss: 159.5732\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 437.7318 - val_loss: 154.3664\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 429.2576 - val_loss: 149.2745\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 420.9157 - val_loss: 144.2957\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 412.7052 - val_loss: 139.4287\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 404.6240 - val_loss: 134.6723\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 396.6714 - val_loss: 130.0248\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 388.8456 - val_loss: 125.4853\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 381.1455 - val_loss: 121.0517\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 373.5694 - val_loss: 116.7234\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 366.1162 - val_loss: 112.4986\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 358.7848 - val_loss: 108.3762\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 351.5735 - val_loss: 104.3547\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 344.4813 - val_loss: 100.4327\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 337.5065 - val_loss: 96.6089\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 330.6480 - val_loss: 92.8822\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 323.9046 - val_loss: 89.2512\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 317.2749 - val_loss: 85.7142\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 310.7574 - val_loss: 82.2705\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 304.3512 - val_loss: 78.9183\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 298.0548 - val_loss: 75.6566\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 291.8669 - val_loss: 72.4839\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 285.7864 - val_loss: 69.3993\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 279.8119 - val_loss: 66.4010\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 273.9421 - val_loss: 63.4880\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 268.1757 - val_loss: 60.6590\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 262.5118 - val_loss: 57.9128\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 256.9488 - val_loss: 55.2481\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 251.4857 - val_loss: 52.6637\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 246.1210 - val_loss: 50.1585\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 240.8539 - val_loss: 47.7307\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 235.6828 - val_loss: 45.3798\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 230.6067 - val_loss: 43.1043\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 225.6243 - val_loss: 40.9027\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 220.7344 - val_loss: 38.7742\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 215.9360 - val_loss: 36.7173\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 211.2276 - val_loss: 34.7311\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 206.6080 - val_loss: 32.8142\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 202.0766 - val_loss: 30.9657\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 197.6317 - val_loss: 29.1840\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 193.2723 - val_loss: 27.4683\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 188.9972 - val_loss: 25.8171\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 184.8052 - val_loss: 24.2296\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 180.6953 - val_loss: 22.7046\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 176.6665 - val_loss: 21.2408\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 172.7175 - val_loss: 19.8374\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 168.8473 - val_loss: 18.4929\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 165.0545 - val_loss: 17.2064\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 161.3381 - val_loss: 15.9768\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 157.6973 - val_loss: 14.8029\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 154.1308 - val_loss: 13.6837\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 150.6374 - val_loss: 12.6181\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 147.2163 - val_loss: 11.6051\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 143.8663 - val_loss: 10.6437\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 140.5863 - val_loss: 9.7325\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 137.3753 - val_loss: 8.8709\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 134.2324 - val_loss: 8.0576\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 131.1563 - val_loss: 7.2916\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 128.1461 - val_loss: 6.5720\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 125.2007 - val_loss: 5.8977\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 122.3192 - val_loss: 5.2676\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 119.5005 - val_loss: 4.6809\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.7437 - val_loss: 4.1365\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.0477 - val_loss: 3.6336\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 111.4118 - val_loss: 3.1710\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 108.8345 - val_loss: 2.7479\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 106.3154 - val_loss: 2.3633\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 103.8532 - val_loss: 2.0162\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 101.4469 - val_loss: 1.7058\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 99.0958 - val_loss: 1.4312\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 96.7990 - val_loss: 1.1914\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 94.5553 - val_loss: 0.9854\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 92.3639 - val_loss: 0.8126\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 90.2240 - val_loss: 0.6719\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 88.1346 - val_loss: 0.5624\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 86.0949 - val_loss: 0.4834\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 84.1038 - val_loss: 0.4340\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 82.1606 - val_loss: 0.4133\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 80.2645 - val_loss: 0.4205\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.4146 - val_loss: 0.4548\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 76.6099 - val_loss: 0.5154\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 74.8497 - val_loss: 0.6015\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 73.1332 - val_loss: 0.7122\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 71.4593 - val_loss: 0.8469\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.8275 - val_loss: 1.0047\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.2369 - val_loss: 1.1849\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 66.6868 - val_loss: 1.3868\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.1761 - val_loss: 1.6094\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.7044 - val_loss: 1.8523\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 62.2708 - val_loss: 2.1146\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 60.8745 - val_loss: 2.3958\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 59.5146 - val_loss: 2.6948\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 58.1906 - val_loss: 3.0113\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 56.9017 - val_loss: 3.3444\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 55.6472 - val_loss: 3.6937\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 54.4263 - val_loss: 4.0582\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 53.2384 - val_loss: 4.4374\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 52.0827 - val_loss: 4.8307\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 50.9586 - val_loss: 5.2376\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 49.8653 - val_loss: 5.6572\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 48.8022 - val_loss: 6.0891\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 47.7687 - val_loss: 6.5327\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 46.7642 - val_loss: 6.9874\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 45.7879 - val_loss: 7.4525\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 44.8392 - val_loss: 7.9277\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 43.9175 - val_loss: 8.4122\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 43.0221 - val_loss: 8.9055\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.1527 - val_loss: 9.4073\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 41.3083 - val_loss: 9.9168\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 40.4886 - val_loss: 10.4336\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 39.6930 - val_loss: 10.9573\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 38.9208 - val_loss: 11.4873\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 38.1715 - val_loss: 12.0232\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.4445 - val_loss: 12.5645\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 36.7393 - val_loss: 13.1107\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 36.0554 - val_loss: 13.6614\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 35.3922 - val_loss: 14.2162\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 34.7493 - val_loss: 14.7745\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 34.1262 - val_loss: 15.3363\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 33.5222 - val_loss: 15.9008\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.9370 - val_loss: 16.4678\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 32.3700 - val_loss: 17.0368\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 31.8209 - val_loss: 17.6075\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 31.2891 - val_loss: 18.1796\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 30.7741 - val_loss: 18.7526\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 30.2755 - val_loss: 19.3260\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.7931 - val_loss: 19.9001\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.3261 - val_loss: 20.4741\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 28.8742 - val_loss: 21.0476\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 28.4371 - val_loss: 21.6205\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 28.0143 - val_loss: 22.1925\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.6055 - val_loss: 22.7634\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 27.2101 - val_loss: 23.3327\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 26.8280 - val_loss: 23.9002\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 26.4586 - val_loss: 24.4657\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26.1017 - val_loss: 25.0289\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 25.7568 - val_loss: 25.5897\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 25.4236 - val_loss: 26.1477\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.1018 - val_loss: 26.7028\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.7910 - val_loss: 27.2545\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 24.4910 - val_loss: 27.8031\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.2014 - val_loss: 28.3481\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.9219 - val_loss: 28.8893\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.6521 - val_loss: 29.4265\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.3918 - val_loss: 29.9597\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.1407 - val_loss: 30.4884\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 22.8985 - val_loss: 31.0127\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.6650 - val_loss: 31.5325\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.4399 - val_loss: 32.0478\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.2227 - val_loss: 32.5579\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.0135 - val_loss: 33.0633\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.8118 - val_loss: 33.5632\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.6176 - val_loss: 34.0579\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.4304 - val_loss: 34.5476\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.2501 - val_loss: 35.0317\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.0765 - val_loss: 35.5102\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.9093 - val_loss: 35.9833\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.7483 - val_loss: 36.4503\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 20.5933 - val_loss: 36.9119\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.4442 - val_loss: 37.3675\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.3007 - val_loss: 37.8173\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.1625 - val_loss: 38.2612\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.0297 - val_loss: 38.6987\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.9019 - val_loss: 39.1307\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.7790 - val_loss: 39.5563\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.6608 - val_loss: 39.9756\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 19.5472 - val_loss: 40.3891\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.4378 - val_loss: 40.7961\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.3328 - val_loss: 41.1972\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 19.2319 - val_loss: 41.5920\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 19.1349 - val_loss: 41.9804\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 19.0417 - val_loss: 42.3629\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.9521 - val_loss: 42.7389\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.8660 - val_loss: 43.1088\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.7834 - val_loss: 43.4726\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.7040 - val_loss: 43.8298\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.6278 - val_loss: 44.1809\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.5546 - val_loss: 44.5258\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.4843 - val_loss: 44.8647\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.4169 - val_loss: 45.1975\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.3521 - val_loss: 45.5238\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.2900 - val_loss: 45.8442\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.2303 - val_loss: 46.1587\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.1730 - val_loss: 46.4674\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.1181 - val_loss: 46.7695\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.0654 - val_loss: 47.0657\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.0149 - val_loss: 47.3566\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 17.9663 - val_loss: 47.6411\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.9198 - val_loss: 47.9197\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 17.8752 - val_loss: 48.1928\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 17.8324 - val_loss: 48.4602\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.7913 - val_loss: 48.7219\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.7519 - val_loss: 48.9781\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.7142 - val_loss: 49.2286\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.6780 - val_loss: 49.4738\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.6433 - val_loss: 49.7136\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.6100 - val_loss: 49.9480\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.5780 - val_loss: 50.1770\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.5474 - val_loss: 50.4008\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.5181 - val_loss: 50.6196\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.4900 - val_loss: 50.8334\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.4630 - val_loss: 51.0418\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 17.4372 - val_loss: 51.2456\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.4124 - val_loss: 51.4443\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.3886 - val_loss: 51.6383\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.3659 - val_loss: 51.8275\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.3440 - val_loss: 52.0122\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.3231 - val_loss: 52.1922\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.3031 - val_loss: 52.3677\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.2838 - val_loss: 52.5386\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.2654 - val_loss: 52.7054\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 17.2478 - val_loss: 52.8678\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.2308 - val_loss: 53.0260\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 17.2146 - val_loss: 53.1799\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.1990 - val_loss: 53.3299\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.1841 - val_loss: 53.4758\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.1698 - val_loss: 53.6178\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 17.1562 - val_loss: 53.7556\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.1430 - val_loss: 53.8903\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.1305 - val_loss: 54.0209\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.1184 - val_loss: 54.1480\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.1068 - val_loss: 54.2711\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.0958 - val_loss: 54.3911\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.0851 - val_loss: 54.5074\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0750 - val_loss: 54.6207\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0652 - val_loss: 54.7305\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.0559 - val_loss: 54.8370\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.0469 - val_loss: 54.9407\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 17.0383 - val_loss: 55.0412\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0301 - val_loss: 55.1389\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0222 - val_loss: 55.2335\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.0146 - val_loss: 55.3249\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0074 - val_loss: 55.4138\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.0004 - val_loss: 55.4997\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 16.9938 - val_loss: 55.5832\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9874 - val_loss: 55.6641\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9812 - val_loss: 55.7422\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9754 - val_loss: 55.8182\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9697 - val_loss: 55.8915\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.9644 - val_loss: 55.9627\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9592 - val_loss: 56.0314\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9542 - val_loss: 56.0980\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9495 - val_loss: 56.1623\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9450 - val_loss: 56.2245\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9406 - val_loss: 56.2846\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9364 - val_loss: 56.3426\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9325 - val_loss: 56.3987\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9286 - val_loss: 56.4531\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9250 - val_loss: 56.5054\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9214 - val_loss: 56.5558\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9181 - val_loss: 56.6046\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9149 - val_loss: 56.6519\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9118 - val_loss: 56.6972\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9089 - val_loss: 56.7411\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9060 - val_loss: 56.7835\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9033 - val_loss: 56.8242\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 16.9008 - val_loss: 56.8635\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8983 - val_loss: 56.9013\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8959 - val_loss: 56.9377\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8937 - val_loss: 56.9730\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8915 - val_loss: 57.0070\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8894 - val_loss: 57.0396\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8875 - val_loss: 57.0712\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8855 - val_loss: 57.1012\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8837 - val_loss: 57.1301\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8821 - val_loss: 57.1580\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8804 - val_loss: 57.1850\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8788 - val_loss: 57.2108\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8773 - val_loss: 57.2356\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8759 - val_loss: 57.2597\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8745 - val_loss: 57.2826\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8732 - val_loss: 57.3046\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8720 - val_loss: 57.3256\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8709 - val_loss: 57.3459\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 16.8697 - val_loss: 57.3653\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8686 - val_loss: 57.3841\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8676 - val_loss: 57.4020\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8667 - val_loss: 57.4191\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8657 - val_loss: 57.4355\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8649 - val_loss: 57.4512\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8640 - val_loss: 57.4663\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8633 - val_loss: 57.4808\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8625 - val_loss: 57.4947\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8618 - val_loss: 57.5079\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8611 - val_loss: 57.5206\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8605 - val_loss: 57.5328\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8599 - val_loss: 57.5443\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8594 - val_loss: 57.5553\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8588 - val_loss: 57.5659\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8583 - val_loss: 57.5759\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.8579 - val_loss: 57.5856\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8574 - val_loss: 57.5951\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8570 - val_loss: 57.6037\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8566 - val_loss: 57.6123\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 16.8562 - val_loss: 57.6202\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8559 - val_loss: 57.6277\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8556 - val_loss: 57.6354\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8553 - val_loss: 57.6422\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8550 - val_loss: 57.6485\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8548 - val_loss: 57.6546\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8546 - val_loss: 57.6606\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8543 - val_loss: 57.6664\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8542 - val_loss: 57.6717\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8540 - val_loss: 57.6769\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8538 - val_loss: 57.6820\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8536 - val_loss: 57.6865\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8535 - val_loss: 57.6910\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8534 - val_loss: 57.6953\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8533 - val_loss: 57.6991\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8532 - val_loss: 57.7030\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8531 - val_loss: 57.7064\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8530 - val_loss: 57.7099\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.8530 - val_loss: 57.7131\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8529 - val_loss: 57.7161\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 16.8529 - val_loss: 57.7190\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8529 - val_loss: 57.7218\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8529 - val_loss: 57.7245\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8528 - val_loss: 57.7268\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8528 - val_loss: 57.7290\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8529 - val_loss: 57.7313\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8529 - val_loss: 57.7334\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8528 - val_loss: 57.7349\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8529 - val_loss: 57.7367\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8530 - val_loss: 57.7385\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8530 - val_loss: 57.7400\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8530 - val_loss: 57.7415\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.8531 - val_loss: 57.7429\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8532 - val_loss: 57.7441\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8532 - val_loss: 57.7456\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8533 - val_loss: 57.7465\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8534 - val_loss: 57.7477\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8534 - val_loss: 57.7485\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.8535 - val_loss: 57.7493\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 16.8536 - val_loss: 57.7503\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 16.8537 - val_loss: 57.7511\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8538 - val_loss: 57.7519\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8539 - val_loss: 57.7526\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8540 - val_loss: 57.7530\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8541 - val_loss: 57.7535\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8542 - val_loss: 57.7541\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8543 - val_loss: 57.7547\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8544 - val_loss: 57.7551\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8545 - val_loss: 57.7553\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8546 - val_loss: 57.7555\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.8548 - val_loss: 57.7558\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.8549 - val_loss: 57.7562\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8550 - val_loss: 57.7564\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8551 - val_loss: 57.7569\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.8553 - val_loss: 57.7571\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 446ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61.22649393, 61.15646592, 61.08643791, 61.0164099 , 60.94638189,\n",
       "        60.87635387, 60.80632586, 60.73629785, 60.66626984, 60.59624183,\n",
       "        60.52621382, 60.45618581, 60.3861578 , 60.31612979, 60.24610177,\n",
       "        60.18085901, 60.1248366 , 60.06881419, 60.01279178, 59.95676937,\n",
       "        59.90074697, 59.84472456, 59.78870215, 59.73267974, 59.67665733,\n",
       "        59.62063492, 59.56461251, 59.5085901 , 59.45256769, 59.39654528,\n",
       "        59.34052288, 59.28450047, 59.22847806, 59.17245565, 59.11643324,\n",
       "        59.06041083, 59.00438842, 58.94836601, 58.8923436 , 58.8363212 ,\n",
       "        58.78029879, 58.72427638, 58.66825397, 58.61223156, 58.55620915,\n",
       "        58.50018674, 58.44416433, 58.38814192, 58.33211951, 58.27609711,\n",
       "        58.2200747 , 58.19101307, 58.17700747, 58.16300187, 58.14899627,\n",
       "        58.13499066, 58.12098506, 58.10697946, 58.09297386, 58.07896825,\n",
       "        58.06496265, 58.05095705, 58.03695145, 58.02294585, 58.00894024,\n",
       "        57.99493464, 57.98092904, 57.96692344, 57.95291783, 57.93891223,\n",
       "        57.92490663, 57.91090103, 57.89689542, 57.88288982, 57.86888422,\n",
       "        57.85487862, 57.84087302, 57.82686741, 57.81286181, 57.79885621,\n",
       "        66.10621643,  0.        ,  0.        ,  0.        ,  0.23122965,\n",
       "         0.71649396,  0.        ,  0.        ,  0.22095743,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.06862688,\n",
       "         1.1428057 ,  0.13909158,  1.00525665,  0.13792966,  0.24003939]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56.85581232, 56.85114379, 56.84647526, 56.84180672, 56.83713819,\n",
       "       56.83246965, 56.82780112, 56.82313259, 56.81846405, 56.81379552,\n",
       "       56.80912698, 56.80445845, 56.79978992, 56.79512138, 56.79045285,\n",
       "       56.78578431, 56.78111578, 56.77644725, 56.77177871, 56.76711018,\n",
       "       56.76244164, 56.75777311, 56.75310458, 56.74843604, 56.74376751,\n",
       "       56.73909897, 56.73443044, 56.7297619 , 56.72509337, 56.72042484,\n",
       "       56.7157563 , 56.71108777, 56.70641923, 56.7017507 , 56.69358077,\n",
       "       56.68330999, 56.67303922, 56.66276844, 56.65249767, 56.64222689,\n",
       "       56.63195612, 56.62168534, 56.61141457, 56.60114379, 56.59087302,\n",
       "       56.58060224, 56.57033147, 56.56006069, 56.54978992, 56.53951914,\n",
       "       56.52924837, 56.51897759, 56.50870682, 56.49843604, 56.48816527,\n",
       "       56.47789449, 56.46762372, 56.45735294, 56.44708217, 56.43681139,\n",
       "       56.42654062, 56.41626984, 56.40599907, 56.39572829, 56.38545752,\n",
       "       56.37518674, 56.36491597, 56.35464519, 56.34437442, 56.33410364,\n",
       "       56.32383287, 56.31356209, 56.30329132, 56.29302054, 56.28274977,\n",
       "       56.27247899, 56.26220822, 56.25193744, 56.24166667, 56.23139589,\n",
       "       56.22112512, 56.21085434, 56.20058357, 56.19031279, 56.18004202,\n",
       "       56.16977124, 56.15950047, 56.14922969, 56.13895892, 56.12868814,\n",
       "       56.11841737, 56.10814659, 56.09787582, 56.08760504, 56.07733427,\n",
       "       56.06706349, 56.05679272, 56.04652194, 56.03625117, 56.02598039])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.503882198712425\n",
      "12.685939348737723\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
