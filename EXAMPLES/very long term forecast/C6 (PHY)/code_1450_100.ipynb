{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1545    55.243627\n",
       "1546    55.237091\n",
       "1547    55.230556\n",
       "1548    55.224020\n",
       "1549    55.217484\n",
       "Name: C6, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1445     1.142806\n",
       "1446     0.139092\n",
       "1447     1.005257\n",
       "1448     0.137930\n",
       "1449     0.240039\n",
       "Name: C6, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 2s 36ms/step - loss: 4178.1553 - val_loss: 3154.3247\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4115.5337 - val_loss: 3121.0549\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4077.3713 - val_loss: 3087.6313\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4029.8860 - val_loss: 3031.0925\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3972.6660 - val_loss: 2981.1387\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3915.4038 - val_loss: 2943.8701\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3872.7903 - val_loss: 2907.0012\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3830.8027 - val_loss: 2870.7812\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3789.4966 - val_loss: 2835.1394\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3748.7827 - val_loss: 2799.9988\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3708.5835 - val_loss: 2765.3044\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3668.8447 - val_loss: 2731.0186\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3629.5293 - val_loss: 2697.1138\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3590.6128 - val_loss: 2663.5723\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3552.0745 - val_loss: 2630.3782\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3513.8994 - val_loss: 2597.5208\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3476.0767 - val_loss: 2564.9907\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3438.5969 - val_loss: 2532.7810\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3401.4519 - val_loss: 2500.8845\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3364.6357 - val_loss: 2469.2959\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3328.1416 - val_loss: 2438.0107\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3291.9653 - val_loss: 2407.0242\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3256.1023 - val_loss: 2376.3330\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3220.5481 - val_loss: 2345.9326\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3185.2991 - val_loss: 2302.3752\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3133.4697 - val_loss: 2269.8167\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3095.3418 - val_loss: 2237.3494\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3057.7668 - val_loss: 2205.5928\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3020.9612 - val_loss: 2174.4773\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 2984.8184 - val_loss: 2143.9077\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2949.2407 - val_loss: 2113.8145\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2914.1594 - val_loss: 2075.1826\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2867.7056 - val_loss: 2043.0178\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2829.7815 - val_loss: 2011.0327\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2792.5334 - val_loss: 1979.8656\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2756.1741 - val_loss: 1949.4318\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2720.5791 - val_loss: 1919.6189\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2685.6321 - val_loss: 1890.3470\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2651.2537 - val_loss: 1861.5587\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2617.3865 - val_loss: 1833.2158\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2583.9907 - val_loss: 1805.2878\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2551.0359 - val_loss: 1777.7528\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2518.4976 - val_loss: 1750.5920\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2486.3577 - val_loss: 1723.7913\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2454.6011 - val_loss: 1697.3387\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2423.2141 - val_loss: 1671.2238\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2392.1863 - val_loss: 1645.4370\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2361.5076 - val_loss: 1619.9705\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2331.1694 - val_loss: 1594.8174\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2301.1648 - val_loss: 1569.9720\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2271.4868 - val_loss: 1545.4272\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2234.0771 - val_loss: 1508.1500\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2195.6960 - val_loss: 1480.9058\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2163.1826 - val_loss: 1454.4165\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2131.5674 - val_loss: 1428.6836\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2100.7566 - val_loss: 1403.5861\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2070.6165 - val_loss: 1379.0294\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2041.0508 - val_loss: 1354.9489\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2011.9939 - val_loss: 1331.3003\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1983.4005 - val_loss: 1308.0518\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1955.2367 - val_loss: 1285.1793\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1927.4771 - val_loss: 1262.6644\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1900.1019 - val_loss: 1240.4913\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1873.0948 - val_loss: 1218.6479\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1846.4430 - val_loss: 1197.1243\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1820.1346 - val_loss: 1175.9104\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1794.1594 - val_loss: 1154.9985\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1768.5093 - val_loss: 1134.3816\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1743.1768 - val_loss: 1114.0538\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1718.1542 - val_loss: 1094.0088\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1693.4365 - val_loss: 1074.2416\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1669.0171 - val_loss: 1054.7468\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1644.8910 - val_loss: 1035.5206\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1621.0532 - val_loss: 1016.5588\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1597.5000 - val_loss: 997.8566\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1574.2262 - val_loss: 979.4112\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1551.2283 - val_loss: 961.2180\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1528.5022 - val_loss: 943.2752\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1506.0444 - val_loss: 925.5784\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1483.8519 - val_loss: 908.1244\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1461.9213 - val_loss: 890.9110\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1440.2488 - val_loss: 873.9342\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1418.8319 - val_loss: 857.1926\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1397.6676 - val_loss: 840.6825\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1376.7534 - val_loss: 824.4019\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1356.0864 - val_loss: 808.3480\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1335.6636 - val_loss: 792.5185\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1315.4829 - val_loss: 776.9105\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1295.5417 - val_loss: 761.5222\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1275.8375 - val_loss: 746.3513\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1256.3682 - val_loss: 731.3950\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1237.1309 - val_loss: 716.6519\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1218.1238 - val_loss: 702.1191\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1199.3442 - val_loss: 687.7952\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1180.7908 - val_loss: 673.6775\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1162.4604 - val_loss: 659.7639\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 1144.3517 - val_loss: 646.0533\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1126.4625 - val_loss: 632.5430\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1108.7909 - val_loss: 619.2310\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1091.3344 - val_loss: 606.1153\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1074.0907 - val_loss: 593.1943\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1057.0591 - val_loss: 580.4662\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1040.2369 - val_loss: 567.9285\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1023.6223 - val_loss: 555.5801\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1007.2137 - val_loss: 543.4186\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 991.0086 - val_loss: 531.4430\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 975.0062 - val_loss: 519.6508\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 959.2039 - val_loss: 508.0402\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 943.6002 - val_loss: 496.6104\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 928.1933 - val_loss: 485.3582\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 912.9814 - val_loss: 474.2833\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 897.9626 - val_loss: 463.3829\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 883.1356 - val_loss: 452.6559\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 868.4986 - val_loss: 442.1003\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 854.0497 - val_loss: 431.7150\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 839.7874 - val_loss: 421.4977\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 825.7102 - val_loss: 411.4471\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 811.8161 - val_loss: 401.5615\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 798.1038 - val_loss: 391.8396\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 784.5715 - val_loss: 382.2792\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 771.2177 - val_loss: 372.8790\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 758.0407 - val_loss: 363.6377\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 745.0390 - val_loss: 354.5530\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 732.2110 - val_loss: 345.6242\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 719.5552 - val_loss: 336.8488\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 707.0698 - val_loss: 328.2268\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 694.7537 - val_loss: 319.7550\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 682.6052 - val_loss: 311.4323\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 670.6226 - val_loss: 303.2579\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 658.8045 - val_loss: 295.2295\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 647.1497 - val_loss: 287.3459\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 635.6564 - val_loss: 279.6063\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 624.3232 - val_loss: 272.0076\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 613.1483 - val_loss: 264.5496\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 602.1306 - val_loss: 257.2305\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 591.2689 - val_loss: 250.0492\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 580.5612 - val_loss: 243.0034\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 570.0062 - val_loss: 236.0923\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 559.6029 - val_loss: 229.3137\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 549.3492 - val_loss: 222.6675\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 539.2443 - val_loss: 216.1514\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 529.2864 - val_loss: 209.7640\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 519.4747 - val_loss: 203.5040\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 509.8066 - val_loss: 197.3698\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 500.2816 - val_loss: 191.3598\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 490.8983 - val_loss: 185.4734\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 481.6549 - val_loss: 179.7087\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 472.5504 - val_loss: 174.0644\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 463.5833 - val_loss: 168.5390\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 454.7525 - val_loss: 163.1313\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 446.0562 - val_loss: 157.8398\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 437.4933 - val_loss: 152.6631\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 429.0625 - val_loss: 147.5998\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 420.7623 - val_loss: 142.6490\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 412.5915 - val_loss: 137.8088\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 404.5487 - val_loss: 133.0783\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 396.6328 - val_loss: 128.4558\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 388.8424 - val_loss: 123.9398\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 381.1759 - val_loss: 119.5301\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 373.6322 - val_loss: 115.2239\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 366.2102 - val_loss: 111.0207\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 358.9084 - val_loss: 106.9190\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 351.7253 - val_loss: 102.9179\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 344.6601 - val_loss: 99.0153\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 337.7111 - val_loss: 95.2105\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 330.8775 - val_loss: 91.5021\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 324.1576 - val_loss: 87.8889\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 317.5503 - val_loss: 84.3695\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 311.0544 - val_loss: 80.9426\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 304.6685 - val_loss: 77.6068\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 298.3916 - val_loss: 74.3612\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 292.2224 - val_loss: 71.2044\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 286.1595 - val_loss: 68.1350\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 280.2018 - val_loss: 65.1518\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 274.3481 - val_loss: 62.2540\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 268.5973 - val_loss: 59.4398\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 262.9481 - val_loss: 56.7081\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 257.3990 - val_loss: 54.0579\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 251.9491 - val_loss: 51.4878\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 246.5972 - val_loss: 48.9967\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 241.3423 - val_loss: 46.5833\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 236.1828 - val_loss: 44.2467\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 231.1180 - val_loss: 41.9852\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 226.1463 - val_loss: 39.7980\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 221.2668 - val_loss: 37.6837\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 216.4782 - val_loss: 35.6413\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 211.7794 - val_loss: 33.6696\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 207.1693 - val_loss: 31.7673\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 202.6467 - val_loss: 29.9336\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 198.2106 - val_loss: 28.1669\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 193.8596 - val_loss: 26.4663\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 189.5929 - val_loss: 24.8306\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 185.4091 - val_loss: 23.2589\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 181.3074 - val_loss: 21.7498\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 177.2865 - val_loss: 20.3022\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 173.3451 - val_loss: 18.9152\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 169.4826 - val_loss: 17.5874\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 165.6975 - val_loss: 16.3181\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 161.9890 - val_loss: 15.1059\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 158.3559 - val_loss: 13.9498\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 154.7970 - val_loss: 12.8487\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 151.3114 - val_loss: 11.8016\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 147.8982 - val_loss: 10.8076\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 144.5561 - val_loss: 9.8653\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 141.2841 - val_loss: 8.9739\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 138.0813 - val_loss: 8.1321\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 134.9465 - val_loss: 7.3393\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 131.8788 - val_loss: 6.5942\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 128.8772 - val_loss: 5.8958\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 125.9406 - val_loss: 5.2431\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 123.0681 - val_loss: 4.6352\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 120.2586 - val_loss: 4.0710\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 117.5114 - val_loss: 3.5495\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 114.8250 - val_loss: 3.0699\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 112.1988 - val_loss: 2.6311\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 109.6318 - val_loss: 2.2322\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 107.1230 - val_loss: 1.8722\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 104.6715 - val_loss: 1.5501\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 102.2763 - val_loss: 1.2652\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 99.9363 - val_loss: 1.0163\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 97.6510 - val_loss: 0.8027\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 95.4192 - val_loss: 0.6234\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 93.2399 - val_loss: 0.4775\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 91.1124 - val_loss: 0.3641\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 89.0358 - val_loss: 0.2824\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 87.0091 - val_loss: 0.2315\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 85.0314 - val_loss: 0.2105\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 83.1018 - val_loss: 0.2186\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 81.2197 - val_loss: 0.2549\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 79.3840 - val_loss: 0.3186\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 77.5940 - val_loss: 0.4089\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 75.8486 - val_loss: 0.5250\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 74.1473 - val_loss: 0.6660\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 72.4890 - val_loss: 0.8313\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 70.8729 - val_loss: 1.0199\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 69.2985 - val_loss: 1.2311\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 67.7647 - val_loss: 1.4642\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 66.2708 - val_loss: 1.7185\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 64.8160 - val_loss: 1.9931\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 63.3995 - val_loss: 2.2872\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 62.0208 - val_loss: 2.6004\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 60.6788 - val_loss: 2.9316\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 59.3729 - val_loss: 3.2805\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 58.1022 - val_loss: 3.6461\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 56.8663 - val_loss: 4.0278\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 55.6642 - val_loss: 4.4250\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 54.4954 - val_loss: 4.8369\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 53.3589 - val_loss: 5.2630\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 52.2542 - val_loss: 5.7026\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 51.1807 - val_loss: 6.1551\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 50.1375 - val_loss: 6.6198\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 49.1241 - val_loss: 7.0962\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 48.1397 - val_loss: 7.5836\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 47.1838 - val_loss: 8.0814\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 46.2557 - val_loss: 8.5892\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 45.3547 - val_loss: 9.1062\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 44.4804 - val_loss: 9.6320\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 43.6319 - val_loss: 10.1661\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.8088 - val_loss: 10.7077\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.0104 - val_loss: 11.2566\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 41.2361 - val_loss: 11.8120\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 40.4853 - val_loss: 12.3737\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 39.7575 - val_loss: 12.9410\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 39.0522 - val_loss: 13.5135\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 38.3688 - val_loss: 14.0905\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 37.7066 - val_loss: 14.6720\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 37.0653 - val_loss: 15.2572\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.4441 - val_loss: 15.8458\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 35.8427 - val_loss: 16.4374\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 35.2606 - val_loss: 17.0314\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 34.6972 - val_loss: 17.6276\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 34.1521 - val_loss: 18.2254\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 33.6247 - val_loss: 18.8246\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 33.1146 - val_loss: 19.4247\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 32.6214 - val_loss: 20.0255\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 32.1445 - val_loss: 20.6265\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.6835 - val_loss: 21.2273\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.2381 - val_loss: 21.8278\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.8077 - val_loss: 22.4274\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.3919 - val_loss: 23.0260\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 29.9904 - val_loss: 23.6230\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 29.6028 - val_loss: 24.2187\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 29.2285 - val_loss: 24.8122\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 28.8672 - val_loss: 25.4036\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.5186 - val_loss: 25.9923\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 28.1823 - val_loss: 26.5783\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 27.8580 - val_loss: 27.1614\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.5451 - val_loss: 27.7411\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 27.2435 - val_loss: 28.3176\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 26.9527 - val_loss: 28.8901\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.6726 - val_loss: 29.4588\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 26.4026 - val_loss: 30.0232\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.1426 - val_loss: 30.5835\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 25.8921 - val_loss: 31.1394\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 25.6509 - val_loss: 31.6904\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 25.4187 - val_loss: 32.2369\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 25.1952 - val_loss: 32.7782\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.9801 - val_loss: 33.3144\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.7732 - val_loss: 33.8453\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.5741 - val_loss: 34.3707\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.3826 - val_loss: 34.8907\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.1986 - val_loss: 35.4050\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 24.0216 - val_loss: 35.9134\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 23.8515 - val_loss: 36.4161\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.6881 - val_loss: 36.9127\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.5311 - val_loss: 37.4033\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.3802 - val_loss: 37.8877\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 23.2354 - val_loss: 38.3657\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.0963 - val_loss: 38.8378\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.9627 - val_loss: 39.3033\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.8346 - val_loss: 39.7623\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 22.7116 - val_loss: 40.2149\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.5936 - val_loss: 40.6610\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 22.4804 - val_loss: 41.1005\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.3718 - val_loss: 41.5333\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.2678 - val_loss: 41.9599\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.1679 - val_loss: 42.3792\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.0723 - val_loss: 42.7920\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.9806 - val_loss: 43.1983\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 21.8927 - val_loss: 43.5980\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 21.8085 - val_loss: 43.9906\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.7279 - val_loss: 44.3769\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.6507 - val_loss: 44.7563\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.5767 - val_loss: 45.1290\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 21.5059 - val_loss: 45.4952\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.4381 - val_loss: 45.8545\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.3733 - val_loss: 46.2073\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.3112 - val_loss: 46.5534\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.2518 - val_loss: 46.8928\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.1950 - val_loss: 47.2259\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 21.1406 - val_loss: 47.5526\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0886 - val_loss: 47.8728\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 21.0388 - val_loss: 48.1865\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.9913 - val_loss: 48.4939\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.9458 - val_loss: 48.7950\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.9023 - val_loss: 49.0896\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.8608 - val_loss: 49.3782\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.8211 - val_loss: 49.6606\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.7831 - val_loss: 49.9367\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.7469 - val_loss: 50.2070\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.7123 - val_loss: 50.4715\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.6791 - val_loss: 50.7297\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.6475 - val_loss: 50.9824\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.6173 - val_loss: 51.2291\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.5884 - val_loss: 51.4698\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.5609 - val_loss: 51.7053\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.5346 - val_loss: 51.9350\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.5095 - val_loss: 52.1597\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.4854 - val_loss: 52.3785\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.4625 - val_loss: 52.5921\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 20.4406 - val_loss: 52.8006\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.4197 - val_loss: 53.0037\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.3998 - val_loss: 53.2017\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.3808 - val_loss: 53.3943\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.3626 - val_loss: 53.5826\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 20.3453 - val_loss: 53.7657\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.3288 - val_loss: 53.9439\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.3130 - val_loss: 54.1176\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2979 - val_loss: 54.2865\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2835 - val_loss: 54.4510\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2698 - val_loss: 54.6113\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2567 - val_loss: 54.7670\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 20.2442 - val_loss: 54.9184\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 20.2322 - val_loss: 55.0654\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2209 - val_loss: 55.2083\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2100 - val_loss: 55.3473\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1996 - val_loss: 55.4822\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1897 - val_loss: 55.6133\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1803 - val_loss: 55.7408\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.1713 - val_loss: 55.8645\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.1627 - val_loss: 55.9845\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 20.1545 - val_loss: 56.1008\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.1466 - val_loss: 56.2138\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1391 - val_loss: 56.3232\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1320 - val_loss: 56.4293\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.1252 - val_loss: 56.5322\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.1187 - val_loss: 56.6320\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 20.1125 - val_loss: 56.7285\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.1066 - val_loss: 56.8218\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.1010 - val_loss: 56.9124\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0956 - val_loss: 57.0001\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.0904 - val_loss: 57.0850\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0855 - val_loss: 57.1671\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0808 - val_loss: 57.2465\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0764 - val_loss: 57.3233\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0721 - val_loss: 57.3975\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0680 - val_loss: 57.4692\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0642 - val_loss: 57.5385\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0605 - val_loss: 57.6057\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0570 - val_loss: 57.6705\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0536 - val_loss: 57.7330\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 20.0504 - val_loss: 57.7934\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0473 - val_loss: 57.8516\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0444 - val_loss: 57.9079\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0416 - val_loss: 57.9618\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0390 - val_loss: 58.0144\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0364 - val_loss: 58.0648\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0340 - val_loss: 58.1136\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0316 - val_loss: 58.1603\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.0295 - val_loss: 58.2054\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0274 - val_loss: 58.2488\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0254 - val_loss: 58.2907\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0236 - val_loss: 58.3311\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0217 - val_loss: 58.3700\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0200 - val_loss: 58.4074\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0183 - val_loss: 58.4432\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0168 - val_loss: 58.4775\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0154 - val_loss: 58.5109\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0139 - val_loss: 58.5427\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0126 - val_loss: 58.5734\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0114 - val_loss: 58.6029\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 20.0101 - val_loss: 58.6311\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0090 - val_loss: 58.6581\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0079 - val_loss: 58.6844\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0069 - val_loss: 58.7095\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0059 - val_loss: 58.7335\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0049 - val_loss: 58.7566\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0041 - val_loss: 58.7785\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0033 - val_loss: 58.7999\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.0025 - val_loss: 58.8199\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0017 - val_loss: 58.8394\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0010 - val_loss: 58.8580\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 20.0004 - val_loss: 58.8759\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9998 - val_loss: 58.8927\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9992 - val_loss: 58.9093\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9986 - val_loss: 58.9247\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9981 - val_loss: 58.9396\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9976 - val_loss: 58.9542\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9971 - val_loss: 58.9678\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9968 - val_loss: 58.9811\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9963 - val_loss: 58.9935\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 19.9959 - val_loss: 59.0055\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 19.9955 - val_loss: 59.0167\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9953 - val_loss: 59.0277\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9950 - val_loss: 59.0384\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9947 - val_loss: 59.0483\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9944 - val_loss: 59.0577\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9942 - val_loss: 59.0668\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9939 - val_loss: 59.0756\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9937 - val_loss: 59.0838\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9935 - val_loss: 59.0914\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9934 - val_loss: 59.0989\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9932 - val_loss: 59.1058\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9931 - val_loss: 59.1124\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9930 - val_loss: 59.1189\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9929 - val_loss: 59.1251\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9928 - val_loss: 59.1308\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9927 - val_loss: 59.1361\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 19.9926 - val_loss: 59.1414\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9926 - val_loss: 59.1464\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9925 - val_loss: 59.1509\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 19.9925 - val_loss: 59.1552\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9925 - val_loss: 59.1594\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9925 - val_loss: 59.1635\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9925 - val_loss: 59.1673\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9925 - val_loss: 59.1704\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9925 - val_loss: 59.1741\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9925 - val_loss: 59.1772\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9925 - val_loss: 59.1803\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9926 - val_loss: 59.1831\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9926 - val_loss: 59.1858\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9927 - val_loss: 59.1884\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9927 - val_loss: 59.1907\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9928 - val_loss: 59.1929\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9928 - val_loss: 59.1949\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9930 - val_loss: 59.1970\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.9930 - val_loss: 59.1987\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9931 - val_loss: 59.2003\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9932 - val_loss: 59.2022\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9933 - val_loss: 59.2037\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 19.9934 - val_loss: 59.2051\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9935 - val_loss: 59.2064\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9936 - val_loss: 59.2078\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9937 - val_loss: 59.2089\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 19.9938 - val_loss: 59.2102\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9939 - val_loss: 59.2111\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9940 - val_loss: 59.2123\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 19.9941 - val_loss: 59.2130\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9943 - val_loss: 59.2138\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9944 - val_loss: 59.2146\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9945 - val_loss: 59.2151\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9946 - val_loss: 59.2158\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9947 - val_loss: 59.2165\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9949 - val_loss: 59.2169\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9950 - val_loss: 59.2172\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9952 - val_loss: 59.2178\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 19.9953 - val_loss: 59.2183\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9954 - val_loss: 59.2187\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9955 - val_loss: 59.2191\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9957 - val_loss: 59.2192\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9958 - val_loss: 59.2193\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9960 - val_loss: 59.2196\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9961 - val_loss: 59.2197\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9963 - val_loss: 59.2198\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9964 - val_loss: 59.2200\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 19.9965 - val_loss: 59.2200\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9967 - val_loss: 59.2200\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9968 - val_loss: 59.2201\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9970 - val_loss: 59.2204\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9971 - val_loss: 59.2205\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 456ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.91537815e+01, 5.90977591e+01, 5.90417367e+01, 5.89857143e+01,\n",
       "        5.89296919e+01, 5.88736695e+01, 5.88176471e+01, 5.87616247e+01,\n",
       "        5.87056022e+01, 5.86495798e+01, 5.85935574e+01, 5.85375350e+01,\n",
       "        5.84815126e+01, 5.84254902e+01, 5.83694678e+01, 5.83134454e+01,\n",
       "        5.82574230e+01, 5.82014006e+01, 5.81863445e+01, 5.81723389e+01,\n",
       "        5.81583333e+01, 5.81443277e+01, 5.81303221e+01, 5.81163165e+01,\n",
       "        5.81023109e+01, 5.80883053e+01, 5.80742997e+01, 5.80602941e+01,\n",
       "        5.80462885e+01, 5.80322829e+01, 5.80182773e+01, 5.80042717e+01,\n",
       "        5.79902661e+01, 5.79762605e+01, 5.79622549e+01, 5.79482493e+01,\n",
       "        5.79342437e+01, 5.79202381e+01, 5.79062325e+01, 5.78922269e+01,\n",
       "        5.78782213e+01, 5.78642157e+01, 5.78502101e+01, 5.78362045e+01,\n",
       "        5.78221989e+01, 5.78081933e+01, 5.77941877e+01, 5.77801821e+01,\n",
       "        5.77661765e+01, 5.77521709e+01, 5.77381653e+01, 5.77241597e+01,\n",
       "        5.77101541e+01, 5.76961485e+01, 5.76821429e+01, 5.76681373e+01,\n",
       "        5.76541316e+01, 5.76401260e+01, 5.76261205e+01, 5.76121148e+01,\n",
       "        5.75981092e+01, 5.75841036e+01, 5.75700980e+01, 5.75560924e+01,\n",
       "        5.75420868e+01, 5.75280812e+01, 5.75140756e+01, 5.75000700e+01,\n",
       "        5.74860644e+01, 5.74720588e+01, 5.74580532e+01, 5.74440476e+01,\n",
       "        5.74300420e+01, 5.74160364e+01, 5.74020308e+01, 5.73880252e+01,\n",
       "        5.73740196e+01, 5.73600140e+01, 5.73460084e+01, 5.73320028e+01,\n",
       "        6.55019760e+01, 1.05346406e+00, 3.07017177e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.41331673e-01, 4.95466381e-01,\n",
       "        0.00000000e+00, 5.24197578e-01, 0.00000000e+00, 4.05044295e-02,\n",
       "        2.80665815e-01, 0.00000000e+00, 0.00000000e+00, 2.30266646e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56.01570962, 56.00543884, 55.99516807, 55.98489729, 55.97462652,\n",
       "       55.96435574, 55.95408497, 55.94381419, 55.93354342, 55.92327264,\n",
       "       55.91300187, 55.90273109, 55.89246032, 55.88218954, 55.87191877,\n",
       "       55.86164799, 55.85137722, 55.84110644, 55.83083567, 55.82056489,\n",
       "       55.81029412, 55.80002334, 55.78975257, 55.77948179, 55.76921102,\n",
       "       55.75894024, 55.74866947, 55.73839869, 55.72812792, 55.71785714,\n",
       "       55.70758637, 55.69731559, 55.68704482, 55.67677404, 55.66650327,\n",
       "       55.65623249, 55.64596172, 55.63569094, 55.62542017, 55.61514939,\n",
       "       55.60487862, 55.59656863, 55.59003268, 55.58349673, 55.57696078,\n",
       "       55.57042484, 55.56388889, 55.55735294, 55.55081699, 55.54428105,\n",
       "       55.5377451 , 55.53120915, 55.5246732 , 55.51813725, 55.51160131,\n",
       "       55.50506536, 55.49852941, 55.49199346, 55.48545752, 55.47892157,\n",
       "       55.47238562, 55.46584967, 55.45931373, 55.45277778, 55.44624183,\n",
       "       55.43970588, 55.43316993, 55.42663399, 55.42009804, 55.41356209,\n",
       "       55.40702614, 55.4004902 , 55.39395425, 55.3874183 , 55.38088235,\n",
       "       55.37434641, 55.36781046, 55.36127451, 55.35473856, 55.34820261,\n",
       "       55.34166667, 55.33513072, 55.32859477, 55.32205882, 55.31552288,\n",
       "       55.30898693, 55.30245098, 55.29591503, 55.28937908, 55.28284314,\n",
       "       55.27630719, 55.26977124, 55.26323529, 55.25669935, 55.2501634 ,\n",
       "       55.24362745, 55.2370915 , 55.23055556, 55.22401961, 55.21748366])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.136289908527143\n",
      "12.42957540328452\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
